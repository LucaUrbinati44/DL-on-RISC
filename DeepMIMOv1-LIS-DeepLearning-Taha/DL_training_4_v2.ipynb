{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4a77e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 13:45:15.161941: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745408715.247721    1202 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745408715.271610    1202 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745408715.430575    1202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745408715.430601    1202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745408715.430604    1202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745408715.430606    1202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Imposta il seed globale\n",
    "seed = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "#from tensorflow.keras.constraints import MinMaxNorm\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Normalization, Dense, Dropout, ReLU, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, TensorBoard, ProgbarLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "import sys\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "# Imposta float32 come tipo predefinito in Keras\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "# Tipo da usare esplicitamente (es. in numpy)\n",
    "force_datatype = np.float32\n",
    "\n",
    "use_gpu = 1\n",
    "\n",
    "# ------------------ Configurazione GPU ------------------ #\n",
    "if not use_gpu:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disabilita la GPU\n",
    "\n",
    "print(f\"Using GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "tf.config.optimizer.set_jit(False)  # XLA off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e2168",
   "metadata": {},
   "source": [
    "## Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70c64d-2e2a-4c78-92da-c841335d0364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIS  32 32\n",
      "Training_Size_dd: 10000\n"
     ]
    }
   ],
   "source": [
    "base_folder = '/mnt/c/Users/Work/Desktop/deepMIMO/RIS/DeepMIMOv1-LIS-DeepLearning-Taha/'\n",
    "input_folder = base_folder + 'Output Matlab/'\n",
    "\n",
    "#DeepMIMO_dataset_folder = input_folder + 'DeepMIMO Dataset/'\n",
    "DL_dataset_folder = input_folder + 'DL Dataset/'\n",
    "network_folder_in = input_folder + 'Neural Network/'\n",
    "\n",
    "output_folder = base_folder + 'Output_Python/'\n",
    "network_folder_out = output_folder + 'Neural_Network/'\n",
    "network_folder_out_YPredicted = output_folder + 'Neural Network/YPredicted/'\n",
    "saved_models = network_folder_out + 'saved_models/'\n",
    "figure_folder = output_folder + 'Figures/'\n",
    "\n",
    "ris=0\n",
    "\n",
    "My_ar = [32, 64]\n",
    "Mz_ar = [32, 64]\n",
    "Mx = 1\n",
    "My = My_ar[ris]\n",
    "Mz = Mz_ar[ris]\n",
    "print('RIS ', My, Mz)\n",
    "\n",
    "M_bar=8\n",
    "Ur_rows = [1000, 1200]\n",
    "#              0    1      2      3      4      5      6\n",
    "Training_Size=[2, 10000, 14000, 18000, 22000, 26000, 30000]\n",
    "#print(Training_Size)\n",
    "Training_Size_dd = Training_Size[1]\n",
    "print(\"Training_Size_dd:\", Training_Size_dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276d680",
   "metadata": {},
   "source": [
    "## Directly import XTrain"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef951642",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#Import .mat files of datasets splits\n",
    "filename_XTrain = DL_dataset_folder + 'XTrain' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '_' + str(Training_Size_dd) + '.mat'\n",
    "filename_YTrain = DL_dataset_folder + 'YTrain' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '_' + str(Training_Size_dd) + '.mat'\n",
    "filename_XValidation = DL_dataset_folder + 'XValidation' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '_' + str(Training_Size_dd) + '.mat'\n",
    "filename_YValidation = DL_dataset_folder + 'YValidation' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '_' + str(Training_Size_dd) + '.mat'\n",
    "\n",
    "print(filename_XTrain)\n",
    "print(filename_YTrain)\n",
    "print(filename_XValidation)\n",
    "print(filename_YValidation)\n",
    "\n",
    "# Load the data using h5py for MATLAB v7.3 files\n",
    "with h5py.File(filename_XTrain, 'r') as f:\n",
    "    X_train = np.array(f['XTrain'][:], dtype=force_datatype)\n",
    "with h5py.File(filename_YTrain, 'r') as f:\n",
    "    Y_train = np.array(f['YTrain'][:], dtype=force_datatype)\n",
    "with h5py.File(filename_XValidation, 'r') as f:\n",
    "    X_val = np.array(f['XValidation'][:], dtype=force_datatype)\n",
    "with h5py.File(filename_YValidation, 'r') as f:\n",
    "    Y_val = np.array(f['YValidation'][:], dtype=force_datatype)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "\n",
    "print(Y_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6901e808",
   "metadata": {},
   "source": [
    "## Load Dataset DL_input_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6b39c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36200, 1, 1, 1024)\n",
      "(36200, 1024, 1, 1)\n",
      "(1, 36200)\n",
      "-0.9966719\n",
      "0.99682105\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "filename_DL_input_reshaped = DL_dataset_folder + 'DL_input_reshaped' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '.mat'\n",
    "filename_DL_output_reshaped = DL_dataset_folder + 'DL_output_reshaped' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '.mat'\n",
    "filename_RandP_all = DL_dataset_folder + 'RandP_all' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '.mat'\n",
    "\n",
    "# Load the data using h5py for MATLAB v7.3 files\n",
    "with h5py.File(filename_DL_input_reshaped, 'r') as f:\n",
    "    DL_input_reshaped = np.array(f['DL_input_reshaped'][:], dtype=force_datatype)\n",
    "with h5py.File(filename_DL_output_reshaped, 'r') as f:\n",
    "    DL_output_reshaped = np.array(f['DL_output_reshaped'][:], dtype=force_datatype)\n",
    "with h5py.File(filename_RandP_all, 'r') as f:\n",
    "    RandP_all = np.array(f['RandP_all'][:], dtype=force_datatype)\n",
    "\n",
    "print(DL_input_reshaped.shape)\n",
    "print(DL_output_reshaped.shape)\n",
    "print(RandP_all.shape)\n",
    "\n",
    "print(np.min(DL_input_reshaped))\n",
    "print(np.max(DL_input_reshaped))\n",
    "print(np.min(DL_output_reshaped))\n",
    "print(np.max(DL_output_reshaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79468c0",
   "metadata": {},
   "source": [
    "## Load Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a45577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6200, 1024, 1, 1)\n",
      "(1, 1, 1024, 6200)\n"
     ]
    }
   ],
   "source": [
    "# Costruzione del nome file\n",
    "filename_DL_output_un_reshaped = DL_dataset_folder + 'DL_output_un_reshaped' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '.mat'\n",
    "\n",
    "# Load the data using h5py for MATLAB v7.3 files\n",
    "with h5py.File(filename_DL_output_un_reshaped, 'r') as f:\n",
    "    # Accesso alla variabile (nome del dataset = nome della variabile in MATLAB)\n",
    "    YValidation_un = np.array(f['DL_output_un_reshaped'], dtype=force_datatype)\n",
    "\n",
    "print(YValidation_un.shape)\n",
    "\n",
    "YValidation_un2 = np.transpose(YValidation_un, (3, 2, 1, 0))  # conversione a (b, z, y, x)\n",
    "print(YValidation_un2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e6ab9",
   "metadata": {},
   "source": [
    "## Dataset split originale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32524c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(6200,)\n",
      "(10000, 1024)\n",
      "(10000, 1024)\n",
      "(6200, 1024)\n",
      "(6200, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the input and output arrays if necessary\n",
    "#X = DL_input_reshaped.reshape(DL_input_reshaped.shape[0], -1).astype(np.float32)\n",
    "#Y = DL_output_reshaped.reshape(DL_output_reshaped.shape[0], -1).astype(np.float32)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "#X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=validation_size / (training_size + validation_size), shuffle=False, random_state=seed)\n",
    "\n",
    "RandP_all2 = np.squeeze(np.array(RandP_all.astype(int))) - 1\n",
    "\n",
    "Training_Ind = RandP_all2[0:Training_Size_dd]\n",
    "\n",
    "Validation_Size = 6200\n",
    "Validation_Ind = RandP_all2[-Validation_Size:]\n",
    "\n",
    "print(Training_Ind.shape)\n",
    "print(Validation_Ind.shape)\n",
    "\n",
    "X_train = np.array(DL_input_reshaped[Training_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "Y_train = np.array(DL_output_reshaped[Training_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "X_val = np.array(DL_input_reshaped[Validation_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "Y_val = np.array(DL_output_reshaped[Validation_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12609fb",
   "metadata": {},
   "source": [
    "## Load Matlab trainedNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8bec588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainedNet_seed0_grid1200_M3232_Mbar8_10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745408723.848135    1202 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13709 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:47:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_unnormalized (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ input_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully1_preFlatten1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully1_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully2_preFlatten1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully2_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully3_preFlatten1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully3_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">16,781,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully4_preFlatten1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully4_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_unnormalized (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ input_ (\u001b[38;5;33mNormalization\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │         \u001b[38;5;34m2,049\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully1_preFlatten1 (\u001b[38;5;33mReshape\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully1_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │     \u001b[38;5;34m1,049,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully2_preFlatten1 (\u001b[38;5;33mReshape\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully2_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4096\u001b[0m)     │     \u001b[38;5;34m4,198,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4096\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4096\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully3_preFlatten1 (\u001b[38;5;33mReshape\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4096\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully3_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4096\u001b[0m)     │    \u001b[38;5;34m16,781,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_2 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4096\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4096\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully4_preFlatten1 (\u001b[38;5;33mReshape\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4096\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully4_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,226,689</span> (100.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,226,689\u001b[0m (100.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,224,640</span> (100.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,224,640\u001b[0m (100.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> (8.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,049\u001b[0m (8.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024, 1, 1)\n",
      "[[[[0.00030797]]\n",
      "\n",
      "  [[0.00030797]]\n",
      "\n",
      "  [[0.00030797]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00030797]]\n",
      "\n",
      "  [[0.00030797]]\n",
      "\n",
      "  [[0.00030797]]]]\n",
      "float32\n",
      "(1, 1024, 1, 1)\n",
      "(1, 1024, 1, 1)\n",
      "Sono 1024 valori di mean tutti uguali, 1024 valori di varianza tutti uguali e 1 epsilon per stabilità numerica.\n"
     ]
    }
   ],
   "source": [
    "# Costruisci il path e il nome del modulo\n",
    "modulename = 'trainedNet' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '_' + str(Training_Size_dd)\n",
    "print(modulename)\n",
    "\n",
    "# Aggiungi la cartella al path (quella che contiene la cartella del modello)\n",
    "sys.path.append(network_folder_in)\n",
    "filename_module = os.path.join(network_folder_in, modulename)\n",
    "\n",
    "# Cambia la working directory temporaneamente\n",
    "old_cwd = os.getcwd()\n",
    "os.chdir(network_folder_in)\n",
    "\n",
    "# Importa il modulo ed esegui il load\n",
    "myModel = importlib.import_module(modulename)\n",
    "#import model as myModel\n",
    "model_mat = myModel.load_model()\n",
    "\n",
    "# Ripristina la working directory originale\n",
    "os.chdir(old_cwd)\n",
    "\n",
    "# Ora puoi usare il modello\n",
    "model_mat.summary()\n",
    "\n",
    "print(model_mat.layers[1].mean.shape) # layers[1] is the Normalization layer\n",
    "print(np.array(model_mat.layers[1].mean))\n",
    "\n",
    "print(model_mat.layers[3].dtype)\n",
    "\n",
    "\n",
    "print(model_mat.layers[1].mean.shape)\n",
    "print(model_mat.layers[1].variance.shape)\n",
    "print(\"Sono 1024 valori di mean tutti uguali, 1024 valori di varianza tutti uguali e 1 epsilon per stabilità numerica.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4a83887",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "layer = model_mat.layers[1]\n",
    "\n",
    "# Parametri registrati\n",
    "print(layer.trainable_weights)          # Es: gamma, beta\n",
    "print(layer.non_trainable_weights)      # Es: mean, variance se registrati\n",
    "\n",
    "# Valori interni (se disponibili)\n",
    "if hasattr(layer, \"mean\"):\n",
    "    print(\"Mean:\", layer.mean.numpy())\n",
    "\n",
    "if hasattr(layer, \"variance\"):\n",
    "    print(\"Variance:\", layer.variance.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1caf4a",
   "metadata": {},
   "source": [
    "## Prediction (!!! Before running, check DL_input_reshap concat comment)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3d4591",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "if Training_Size_dd != 30000:\n",
    "    print(\"Fig 7 is only for Training_Size_dd = 30000\")\n",
    "else:\n",
    "    # ------------------ DL Model Prediction ------------------ #\n",
    "    print(\"Start DL prediction for Figure 7...\")\n",
    "\n",
    "    # Concatena XTrain + XValidation, cioè utilizza DL_input_reshaped\n",
    "    #DL_input_reshaped = tf.concat([X_train, X_val], axis=0)\n",
    "    print(DL_input_reshaped.shape)  # (36200, 1, 1, 1024)\n",
    "\n",
    "    DL_input_reshaped = tf.transpose(DL_input_reshaped, perm=[0, 3, 1, 2]) \n",
    "    print(DL_input_reshaped.shape)  # (36200, 1024, 1, 1)\n",
    "\n",
    "    # Esegui predizione con DL_input_reshaped\n",
    "    YPredictedFig7_mat = model_mat.predict(DL_input_reshaped, batch_size=1, verbose=1)\n",
    "    print(YPredictedFig7_mat.shape)\n",
    "\n",
    "    # Recupera gli indici dei codebook\n",
    "    #Indmax_DL_mat = np.argmax(YPredictedFig7_mat, axis=1)\n",
    "    #print(Indmax_DL_mat.shape) # (36200, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f63d500",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "if Training_Size_dd != 30000:\n",
    "    print(\"Fig 7 is only for Training_Size_dd = 30000\")\n",
    "else:\n",
    "    filename_YPredictedFig7_mat = network_folder_out_YPredicted + 'YPredictedFig7_mat' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '_' + str(Training_Size_dd+Validation_Size) + '.mat'\n",
    "\n",
    "    # Scrittura in formato HDF5 (compatibile MATLAB v7.3)\n",
    "    with h5py.File(filename_YPredictedFig7_mat, 'w') as f:\n",
    "        f.create_dataset('YPredictedFig7_mat', data=YPredictedFig7_mat)\n",
    "\n",
    "    print(f\"Variabile salvata in {filename_YPredictedFig7_mat}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "632b25ca",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "print(\"Start DL prediction for Figure 12...\")\n",
    "\n",
    "print(X_val.shape)  # (6200, 1, 1, 1024)\n",
    "\n",
    "X_val_reshaped = tf.transpose(X_val, perm=[0, 3, 1, 2]) \n",
    "print(X_val_reshaped.shape)  # (6200, 1024, 1, 1)\n",
    "\n",
    "# Esegui predizione con DL_input_reshaped\n",
    "YPredicted_mat = model_mat.predict(X_val_reshaped, batch_size=1, verbose=1)\n",
    "print(YPredicted_mat.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b94cb5c0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "filename_YPredicted_mat = network_folder_out_YPredicted + 'YPredicted_mat' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '_' + str(Training_Size_dd) + '.mat'\n",
    "\n",
    "# Scrittura in formato HDF5 (compatibile MATLAB v7.3)\n",
    "with h5py.File(filename_YPredicted_mat, 'w') as f:\n",
    "    f.create_dataset('YPredicted_mat', data=YPredicted_mat)\n",
    "\n",
    "print(f\"Variabile salvata in {filename_YPredicted_mat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad2906",
   "metadata": {},
   "source": [
    "## Recreate the same network in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b379e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the input and output arrays if necessary\n",
    "#X = DL_input_reshaped.reshape(DL_input_reshaped.shape[0], -1).astype(np.float32)\n",
    "#Y = DL_output_reshaped.reshape(DL_output_reshaped.shape[0], -1).astype(np.float32)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "#X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=validation_size / (training_size + validation_size), shuffle=False, random_state=seed)\n",
    "#XTrain = np.array(DL_input_reshaped[:, 0, 0, Training_Ind], dtype=np.float32)\n",
    "#YTrain = np.array(DL_output_reshaped[0, 0, :, Training_Ind], dtype=np.float32)\n",
    "#XValidation = np.array(DL_input_reshaped[:, 0, 0, Validation_Ind], dtype=np.float32)\n",
    "#YValidation = np.array(DL_output_reshaped[0, 0, :, Validation_Ind], dtype=np.float32)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f282e",
   "metadata": {},
   "source": [
    "### Normalize the training data (zero-center normalization) with Scikit-learn\n",
    "\n",
    "✅ 1. Normalization Layer (Keras)\n",
    "\n",
    "✔️ Pro\n",
    "- Incluso nel modello: la normalizzazione fa parte della rete, quindi viene salvata nel .h5 o .pb.\n",
    "\n",
    "- Usato sia in training che in inference, senza fare altro.\n",
    "\n",
    "- Funziona bene anche con serving, export in TensorFlow Lite, TensorFlow.js, ecc.\n",
    "\n",
    "- Può essere adattato con .adapt() usando un tf.data.Dataset o numpy.\n",
    "\n",
    "❌ Contro\n",
    "- Supporta solo alcune strategie di normalizzazione (es. mean-std su ogni feature).\n",
    "\n",
    "- Meno flessibile se hai bisogno di strategie avanzate (e.g. quantile scaling, robust scaling).\n",
    "\n",
    "✅ 2. StandardScaler (Scikit-learn)\n",
    "\n",
    "✔️ Pro\n",
    "- Più flessibile: puoi usare altri scaler (MinMaxScaler, RobustScaler, ecc.).\n",
    "\n",
    "- Ideale se il preprocessing viene gestito fuori dal modello (es. in un pipeline scikit-learn, o se non stai usando solo TensorFlow).\n",
    "\n",
    "❌ Contro\n",
    "- La normalizzazione non è inclusa nel modello.\n",
    "\n",
    "- Devi salvare il scaler a parte e applicarlo manualmente in fase di inference.\n",
    "\n",
    "- Rischio di mismatch tra training e inference se dimentichi qualcosa (es. diverse medie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76425d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6b27c2f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "# Documentation StandardScaler: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "#  with_mean: default=True, If True, center the data before scaling. \n",
    "#  with_std: default=True, If True, scale the data to unit variance (or equivalently, unit standard deviation).\n",
    "# Calcola la media e la deviazione standard per ogni feature (colonna) considerando tutti i campioni (lungo l'asse 0).\n",
    "scaler = StandardScaler(with_std=False) # To match Matlab imageInputLayer normalization behavior\n",
    "\n",
    "X_train_sq = np.squeeze(X_train)\n",
    "X_val_sq = np.squeeze(X_val)\n",
    "\n",
    "X_train_sq_scaled = scaler.fit_transform(X_train_sq)\n",
    "print(f\"Scaler: scale: {scaler.scale_}, mean: {scaler.mean_}, var: {scaler.var_}, n_features_in_: {scaler.n_features_in_}, n_samples_seen_: {scaler.n_samples_seen_}\")\n",
    "\n",
    "# Apply the same scaling to the validation data\n",
    "X_val_sq_scaled = scaler.transform(X_val_sq)  # Transform the validation data using the same scaler\n",
    "\n",
    "X_train_scaled = np.expand_dims(np.expand_dims(X_train_sq_scaled, axis=1), axis=2)\n",
    "X_val_scaled = np.expand_dims(np.expand_dims(X_val_sq_scaled, axis=1), axis=2)\n",
    "print(X_train_scaled.shape)\n",
    "print(X_val_scaled.shape)\n",
    "\n",
    "\n",
    "# Dovrebbe venire:                         -5.1904644e-06\n",
    "print(np.mean(scaler.mean_))             # -5.190480343233745e-06\n",
    "print(np.float32(np.mean(scaler.mean_))) # -5.1904804e-06\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf48909",
   "metadata": {},
   "source": [
    "### Load normalization parameters from Matlab trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f61bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "0.00030796975\n",
      "(1024,)\n",
      "0.00030796975\n",
      "(1024,)\n",
      "1.0\n",
      "[0.00030797 0.00030797 0.00030797 ... 0.00030797 0.00030797 0.00030797]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "filename_trainedNet_scaler = network_folder_in + 'trainedNet_scaler' + '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar) + '_' + str(Training_Size_dd) + '.mat'\n",
    "\n",
    "with h5py.File(filename_trainedNet_scaler, 'r') as f:\n",
    "    trainedNet_scaler = f['trainedNet_scaler'][:][0][0]\n",
    "\n",
    "print(trainedNet_scaler.shape)\n",
    "print(trainedNet_scaler) # should be -5.1904644e-06 for Training_Size_dd=30000\n",
    "\n",
    "\n",
    "# This layer will shift and scale inputs into a distribution centered around 0 with standard deviation 1. \n",
    "# It accomplishes this by precomputing the mean and variance of the data, and calling (input - mean) / sqrt(var) at runtime.\n",
    "# The mean and variance values for the layer must be either supplied on construction or learned via adapt(). \n",
    "# adapt() will compute the mean and variance of the data and store them as the layer's weights. \n",
    "# adapt() should be called before fit(), evaluate(), or predict().\n",
    "\n",
    "mean_array = np.array([trainedNet_scaler]*X_train.shape[1], dtype=force_datatype)\n",
    "variance_array =  np.array([1]*X_train.shape[1], dtype=force_datatype)\n",
    "print(mean_array.shape)\n",
    "print(mean_array[0])\n",
    "print(variance_array.shape)\n",
    "print(variance_array[0])\n",
    "\n",
    "normalizer = Normalization(axis=1, mean=mean_array, variance=variance_array, name='normalization')\n",
    "\n",
    "print(normalizer.input_mean)\n",
    "print(normalizer.input_variance)\n",
    "\n",
    "normalized = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0ce93",
   "metadata": {},
   "source": [
    "### Compute normalization parameters from Training Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizer = Normalization(axis=3, name='normalization')\n",
    "#normalizer.adapt(X_train)   # adapt è richiesto da Normalization prima di fit, ma dopo aver costriuito il modello\n",
    "# ma poichè vogliamo solo la mean e varianza = 1, non possiamo usarlo\n",
    "\n",
    "# Possiamo ovviare con questo trucco\n",
    "mean_array = np.array([np.mean(np.mean(X_train, axis=1))]*X_train.shape[1], dtype=force_datatype)\n",
    "variance_array =  np.array([1]*X_train.shape[1], dtype=force_datatype)\n",
    "print(mean_array.shape)\n",
    "print(mean_array[0])\n",
    "print(variance_array.shape)\n",
    "print(variance_array[0])\n",
    "\n",
    "normalizer = Normalization(axis=1, mean=mean_array, variance=variance_array, name='normalization')\n",
    "\n",
    "m = normalizer.input_mean\n",
    "v = normalizer.input_variance\n",
    "\n",
    "print(m.shape)\n",
    "print(m[0])\n",
    "print(v.shape)\n",
    "print(v[0])\n",
    "\n",
    "normalized = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a8645",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e91ac4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00030797 0.00030797 0.00030797 ... 0.00030797 0.00030797 0.00030797]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Normalizzazione manuale se già hai mean_array e variance_array da MATLAB\n",
    "X_train_normalized = np.array((X_train - mean_array) / np.sqrt(variance_array), dtype=force_datatype)\n",
    "\n",
    "X_val_normalized = np.array((X_val - mean_array) / np.sqrt(variance_array), dtype=force_datatype)\n",
    "\n",
    "print(mean_array)\n",
    "print(variance_array)\n",
    "\n",
    "normalized = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e9c1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3836985   0.22742581 -0.14068858  0.42388362 -0.42758086]\n",
      "[0.00030797 0.00030797 0.00030797 0.00030797 0.00030797]\n",
      "[-0.38400647  0.22711784 -0.14099655  0.42357564 -0.42788884]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][0:5])\n",
    "print(mean_array[0:5])\n",
    "print(X_train_normalized[0][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f869839",
   "metadata": {},
   "source": [
    "## DL Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62c7c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalized == 1:\n",
    "    xt = X_train_normalized\n",
    "    xv = X_val_normalized\n",
    "else:\n",
    "    xt = X_train\n",
    "    xv = X_val"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f416837",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# OLD model\n",
    "\n",
    "#bc = MinMaxNorm(min_value=0.0, max_value=float('inf'))\n",
    "\n",
    "# Define the neural network architecture\n",
    "model_py = Sequential([\n",
    "    #Input(shape=(1, 1, X_train.shape[3]), name='input'),\n",
    "    Input(shape=(X_train.shape[1],), name='input'),\n",
    "\n",
    "    #normalizer, # non mostra i parametri del Normalization layer in model.summary(), ma mi permette di caricare i mean e variance voluti\n",
    "    #Normalization(axis=3, name='normalization'), # mostra i parametri del Normalization layer in model.summary()\n",
    "    # ma richiede la loro inizializzazione dopo la definizione del modello con adapt, ma ottengo una media e variana diverse da quelle attese.\n",
    "    # Si potrebbero forzare i valori dopo la creazione del modello, ma non riesco. \n",
    "    # Poichè già conosco i valori da dare a mean e variance, mi accontento di fissarli prima della creazione del modello.\n",
    "\n",
    "    Dense(units=Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully1_'),\n",
    "    #Dense(units=Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully1_', bias_constraint=bc),\n",
    "    #Dense(units=Y_train.shape[1], kernel_regularizer=None, name='Fully1'),\n",
    "    ReLU(name='relu1'),\n",
    "    Dropout(0.5, name='dropout1'),\n",
    "\n",
    "    Dense(units=4 * Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully2_'),\n",
    "    #Dense(units=4 * Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully2_', bias_constraint=bc),\n",
    "    #Dense(units=4 * Y_train.shape[1], kernel_regularizer=None, name='Fully2'),\n",
    "    ReLU(name='relu2'),\n",
    "    Dropout(0.5, name='dropout2'),\n",
    "\n",
    "    Dense(units=4 * Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully3_'),\n",
    "    #Dense(units=4 * Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully3_', bias_constraint=bc),\n",
    "    #Dense(units=4 * Y_train.shape[1], kernel_regularizer=None, name='Fully3'),\n",
    "    ReLU(name='relu3'),\n",
    "    Dropout(0.5, name='dropout3'),\n",
    "\n",
    "    Dense(units=Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully4_'),\n",
    "    #Dense(units=Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully4_', bias_constraint=bc),\n",
    "    #Dense(units=Y_train.shape[1], kernel_regularizer=None, name='Fully4'),\n",
    "    \n",
    "    #Flatten(name='Flatten')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9ccd1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Fully1_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully2_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully3_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">16,781,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully4_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Fully1_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m1,049,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu1 (\u001b[38;5;33mReLU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully2_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │     \u001b[38;5;34m4,198,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu2 (\u001b[38;5;33mReLU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully3_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │    \u001b[38;5;34m16,781,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu3 (\u001b[38;5;33mReLU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully4_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,224,640</span> (100.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,224,640\u001b[0m (100.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,224,640</span> (100.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,224,640\u001b[0m (100.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function mse_custom at 0x7fdc8c2f34c0>\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "model_py = Sequential([\n",
    "    Input(shape=(X_train.shape[1],), name='input'),\n",
    "\n",
    "    Dense(units=Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully1_'),\n",
    "    ReLU(name='relu1'),\n",
    "    Dropout(0.5, name='dropout1'),\n",
    "\n",
    "    Dense(units=4 * Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully2_'),\n",
    "    ReLU(name='relu2'),\n",
    "    Dropout(0.5, name='dropout2'),\n",
    "\n",
    "    Dense(units=4 * Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully3_'),\n",
    "    ReLU(name='relu3'),\n",
    "    Dropout(0.5, name='dropout3'),\n",
    "\n",
    "    Dense(units=Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully4_'),\n",
    "])\n",
    "\n",
    "# Compile the model with SGD optimizer and mean squared error loss\n",
    "optimizer = SGD(learning_rate=1e-1, momentum=0.9)\n",
    "\n",
    "def mse_keras(y_true, y_pred): # Not working\n",
    "    squared_error = tf.square(y_true - y_pred)  # shape: (batch_size, output_dim)=6200,1024\n",
    "    loss = tf.reduce_mean(squared_error) # scalar\n",
    "    return loss\n",
    "\n",
    "def mse_custom(y_true, y_pred):\n",
    "    # Calcola l'errore quadratico tra vero e predetto\n",
    "    squared_error = tf.square(y_true - y_pred)  # shape: (batch_size, output_dim)=6200,1024\n",
    "\n",
    "    # Somma degli errori lungo l'ultima dimensione (output_dim)\n",
    "    sum_squared_error = tf.reduce_sum(squared_error, axis=-1)  # shape: (batch_size,)=6200\n",
    "\n",
    "    # Media su tutto il batch\n",
    "    loss = 0.5 * tf.reduce_mean(sum_squared_error)  # scalar\n",
    "    return loss\n",
    "\n",
    "def mse_matlab(y_true, y_pred): # Not working\n",
    "    # A regression layer computes the half-mean-squared-error loss for regression tasks:\n",
    "    # https://www.mathworks.com/help//releases/R2021a/deeplearning/ref/regressionlayer.html?searchHighlight=regressionLayer&searchResultIndex=1\n",
    "    squared_error = tf.square(y_true - y_pred)  # shape: (batch_size, output_dim)=6200,1024\n",
    "    \n",
    "    loss = 0.5 * tf.reduce_mean(squared_error)\n",
    "    return loss\n",
    "\n",
    "#model_py.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
    "#model_py.compile(optimizer=optimizer, loss=mse_keras, metrics=['mse'])\n",
    "#model_py.compile(optimizer=optimizer, loss=mse_matlab, metrics=['mse'])\n",
    "model_py.compile(optimizer=optimizer, loss=mse_custom, metrics=['mse'])\n",
    "model_py.summary()\n",
    "\n",
    "print(model_py.loss)\n",
    "\n",
    "# ------------------ Learning Rate Scheduler ------------------ #\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 0 and epoch % 5 == 0: # Prima era modulo 3\n",
    "        return lr * 0.5  # Drop learning rate by factor of 0.5 every x epochs\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,         # Riduce di metà\n",
    "    patience=2,         # Numero di epoche senza miglioramento ≥ y\n",
    "    min_delta=0.1,        # Miglioramento minimo da considerare significativo\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcf44265",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Simula un batch di input per test (può essere anche uno solo)\n",
    "X_prova = xt[:10000]  # shape: (1, 1, 1, F)\n",
    "print(X_prova.shape)\n",
    "\n",
    "# Passa l'input attraverso il layer di normalizzazione\n",
    "X_normalized = normalizer_layer(X_prova)\n",
    "\n",
    "# Verifica: la media dovrebbe essere circa 0\n",
    "mean_result = tf.reduce_mean(X_normalized).numpy()\n",
    "var_result = tf.math.reduce_variance(X_normalized).numpy()\n",
    "\n",
    "print(f\"Media del tensore normalizzato: {mean_result:.5f}\")\n",
    "print(f\"Varianza del tensore normalizzato: {var_result:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9a7226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalized == 0:\n",
    "\n",
    "    # adapt() funziona solo se il layer di normalizzazione non è stato inizializzato con mean e variance\n",
    "\n",
    "    # Supponiamo che X_train abbia shape (samples, 1024, 1, 1)\n",
    "    normalizer_layer = model_py.layers[0]\n",
    "    normalizer_layer.adapt(tf.convert_to_tensor(xt, dtype=tf.float32))  # Normalizzazione su training data\n",
    "\n",
    "    m = np.array(model_py.layers[0].mean)\n",
    "    v = np.array(model_py.layers[0].variance)\n",
    "\n",
    "    print(m.shape)\n",
    "    print(np.max(m))\n",
    "    print(np.min(m))\n",
    "\n",
    "    print(v.shape)\n",
    "    print(np.max(v))\n",
    "    print(np.min(v))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "685fa519",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "model_py.layers[0].set_weights([mean_array, variance_array, np.array(1.0)])\n",
    "model_py.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "m = np.array(model_py.layers[0].mean)\n",
    "v = np.array(model_py.layers[0].variance)\n",
    "\n",
    "print(m.shape)\n",
    "print(np.max(m))\n",
    "\n",
    "print(v.shape)\n",
    "print(np.max(v))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4e8c961",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "model_py.layers[0] = Normalization(axis=3, mean=[np.mean(np.mean(X_train, axis=3))]*X_train.shape[3], variance=[1]*X_train.shape[3], name='normalization')\n",
    "model_py.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "m = np.array(model_py.layers[0].mean)\n",
    "v = np.array(model_py.layers[0].variance)\n",
    "\n",
    "print(m.shape)\n",
    "print(np.max(m))\n",
    "\n",
    "print(v.shape)\n",
    "print(np.max(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c2511d",
   "metadata": {},
   "source": [
    "## DL Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04d7f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'original12-0.5-new-'\n",
    "tensorboard_logs = network_folder_out + 'tensorboard_logs/'+model_type\n",
    "\n",
    "# ------------------ Callback ------------------ #\n",
    "tensorboard_callback = TensorBoard(log_dir=tensorboard_logs, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5edc9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 67244), started 0:07:53 ago. (Use '!kill 67244' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5a92118719c78df4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5a92118719c78df4\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#!tensorboad --logdir /mnt/c/Users/Work/Desktop/deepMIMO/RIS/DeepMIMOv1-LIS-DeepLearning-Taha/Output_Python/Neural_Network/tensorboard_logs --port=6006\n",
    "%tensorboard --logdir=/mnt/c/Users/Work/Desktop/deepMIMO/RIS/DeepMIMOv1-LIS-DeepLearning-Taha/Output_Python/Neural_Network/tensorboard_logs --port=6006 --host=localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ead0f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start DL training...\n",
      "Epoch 1/20\n",
      "20/20 - 3s - 126ms/step - loss: 3.0244 - mse: 0.0041 - val_loss: 2.5590 - val_mse: 0.0032 - learning_rate: 0.0250\n",
      "Epoch 2/20\n",
      "20/20 - 1s - 70ms/step - loss: 3.0051 - mse: 0.0041 - val_loss: 2.5249 - val_mse: 0.0032 - learning_rate: 0.0250\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "20/20 - 1s - 58ms/step - loss: 2.9878 - mse: 0.0041 - val_loss: 2.5031 - val_mse: 0.0031 - learning_rate: 0.0250\n",
      "Epoch 4/20\n",
      "20/20 - 2s - 77ms/step - loss: 2.9608 - mse: 0.0040 - val_loss: 2.5089 - val_mse: 0.0031 - learning_rate: 0.0125\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "20/20 - 1s - 60ms/step - loss: 2.9403 - mse: 0.0040 - val_loss: 2.4814 - val_mse: 0.0031 - learning_rate: 0.0125\n",
      "Epoch 6/20\n",
      "20/20 - 2s - 85ms/step - loss: 2.9124 - mse: 0.0039 - val_loss: 2.4689 - val_mse: 0.0031 - learning_rate: 0.0063\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "20/20 - 1s - 59ms/step - loss: 2.9081 - mse: 0.0039 - val_loss: 2.4665 - val_mse: 0.0031 - learning_rate: 0.0063\n",
      "Epoch 8/20\n",
      "20/20 - 1s - 62ms/step - loss: 2.9061 - mse: 0.0039 - val_loss: 2.4571 - val_mse: 0.0030 - learning_rate: 0.0031\n",
      "Epoch 9/20\n",
      "20/20 - 1s - 58ms/step - loss: 2.9199 - mse: 0.0039 - val_loss: 2.4562 - val_mse: 0.0030 - learning_rate: 0.0031\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "20/20 - 1s - 57ms/step - loss: 2.9020 - mse: 0.0039 - val_loss: 2.4567 - val_mse: 0.0030 - learning_rate: 0.0031\n",
      "Epoch 11/20\n",
      "20/20 - 1s - 57ms/step - loss: 2.9039 - mse: 0.0039 - val_loss: 2.4537 - val_mse: 0.0030 - learning_rate: 0.0016\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0007812500116415322.\n",
      "20/20 - 1s - 57ms/step - loss: 2.8971 - mse: 0.0039 - val_loss: 2.4509 - val_mse: 0.0030 - learning_rate: 0.0016\n",
      "Epoch 13/20\n",
      "20/20 - 1s - 58ms/step - loss: 2.8982 - mse: 0.0039 - val_loss: 2.4517 - val_mse: 0.0030 - learning_rate: 7.8125e-04\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0003906250058207661.\n",
      "20/20 - 1s - 58ms/step - loss: 2.9033 - mse: 0.0039 - val_loss: 2.4511 - val_mse: 0.0030 - learning_rate: 7.8125e-04\n",
      "Epoch 15/20\n",
      "20/20 - 1s - 63ms/step - loss: 2.9012 - mse: 0.0039 - val_loss: 2.4494 - val_mse: 0.0030 - learning_rate: 3.9063e-04\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00019531250291038305.\n",
      "20/20 - 1s - 58ms/step - loss: 2.8942 - mse: 0.0039 - val_loss: 2.4483 - val_mse: 0.0030 - learning_rate: 3.9063e-04\n",
      "Epoch 17/20\n",
      "20/20 - 2s - 78ms/step - loss: 2.8990 - mse: 0.0039 - val_loss: 2.4477 - val_mse: 0.0030 - learning_rate: 1.9531e-04\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.765625145519152e-05.\n",
      "20/20 - 1s - 56ms/step - loss: 2.8996 - mse: 0.0039 - val_loss: 2.4470 - val_mse: 0.0030 - learning_rate: 1.9531e-04\n",
      "Epoch 19/20\n",
      "20/20 - 2s - 92ms/step - loss: 2.9031 - mse: 0.0039 - val_loss: 2.4469 - val_mse: 0.0030 - learning_rate: 9.7656e-05\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.882812572759576e-05.\n",
      "20/20 - 1s - 56ms/step - loss: 2.8819 - mse: 0.0039 - val_loss: 2.4471 - val_mse: 0.0030 - learning_rate: 9.7656e-05\n",
      "Training completed in 0.45 minutes.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Training Options ------------------ #\n",
    "max_epochs = 20\n",
    "mini_batch_size = 500\n",
    "\n",
    "#if Training_Size_dd < mini_batch_size:\n",
    "#    validationFrequency = Training_Size_dd\n",
    "#else:\n",
    "#    validationFrequency = int(np.floor(Training_Size_dd/mini_batch_size))\n",
    "validationFrequency = 1\n",
    "        \n",
    "# ------------------ DL Model Training ------------------ #\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices((x, Y_train)).batch(mini_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "#val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(mini_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Start DL training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history = model_py.fit(\n",
    "    xt, Y_train,\n",
    "    validation_data=(xv, Y_val),\n",
    "    #train_dataset, \n",
    "    #validation_data=val_dataset\n",
    "    batch_size=mini_batch_size,\n",
    "    epochs=max_epochs,\n",
    "    shuffle=True,  # Shuffle data at each epoch\n",
    "    callbacks=[lr_scheduler, tensorboard_callback],\n",
    "    validation_freq=validationFrequency,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Training completed in {elapsed_time / 60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8678325e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Matlab output\n",
    "\n",
    "TrainingLoss: 2.927125930786133\n",
    "ValidationLoss: 2.5600013732910156\n",
    "FinalValidationLoss: 2.5600014\n",
    "\n",
    "TrainingRMSE: 2.419555902481079\n",
    "ValidationRMSE: 2.262742280960083\n",
    "FinalValidationRMSE: 2.2627423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53c37dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in /mnt/c/Users/Work/Desktop/deepMIMO/RIS/DeepMIMOv1-LIS-DeepLearning-Taha/Output_Python/Neural_Network/saved_models/\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model_py.save(saved_models + 'model_' + model_type + '_' + str(Training_Size_dd) + '.keras')  # The file needs to end with the .keras extension\n",
    "\n",
    "print(f\"Model saved in {saved_models}\")\n",
    "\n",
    "#np.save(os.path.join(output_folder, 'history.npy'), history.history)\n",
    "#np.save(os.path.join(output_folder, 'Y_predicted.npy'), Y_predicted)\n",
    "#print(\"History and Y_predicted saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f303cc4",
   "metadata": {},
   "source": [
    "## DL Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f78257",
   "metadata": {},
   "source": [
    "# SOSTITUIRE X_val CON X_test!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dab93762",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90566cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6200, 1024)\n",
      "(6200, 1, 1, 1024)\n",
      "(6200, 1024, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work_wsl/miniconda3/envs/deepmimo/lib/python3.12/site-packages/keras/src/models/functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_unnormalized']\n",
      "Received: inputs=Tensor(shape=(128, 1024, 1, 1))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work_wsl/miniconda3/envs/deepmimo/lib/python3.12/site-packages/keras/src/models/functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_unnormalized']\n",
      "Received: inputs=Tensor(shape=(None, 1024, 1, 1))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step\n",
      "(6200, 1024)\n",
      "(6200,)\n",
      "503\n",
      "542\n"
     ]
    }
   ],
   "source": [
    "if sanity == 1:\n",
    "    \n",
    "    # Sanity check modello matlab importato in python\n",
    "\n",
    "    print(xv.shape)  # (6200, 1024)\n",
    "\n",
    "    xv_reshaped = np.reshape(xv, (xv.shape[0], 1, 1, xv.shape[1]))\n",
    "    print(xv_reshaped.shape)  # Output: (6200, 1, 1, 1024)\n",
    "\n",
    "    X_val_reshaped = tf.transpose(xv_reshaped, perm=[0, 3, 1, 2]) \n",
    "    print(X_val_reshaped.shape)  # (6200, 1024, 1, 1)\n",
    "\n",
    "    # Esegui predizione con DL_input_reshaped\n",
    "    YPredicted_mat = model_mat.predict(X_val_reshaped, batch_size=128, verbose=1)\n",
    "    print(YPredicted_mat.shape)\n",
    "\n",
    "    Indmax_DL_mat = np.argmax(YPredicted_mat, axis=1)\n",
    "    print(Indmax_DL_mat.shape)\n",
    "    print(np.min(Indmax_DL_mat))\n",
    "    print(np.max(Indmax_DL_mat))\n",
    "\n",
    "    Indmax_DL = Indmax_DL_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ffa6ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start DL prediction...\n",
      "(6200, 1024)\n",
      "\u001b[1m 1/49\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 31ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "(6200, 1024)\n",
      "(6200,)\n",
      "501\n",
      "543\n"
     ]
    }
   ],
   "source": [
    "print(\"Start DL prediction...\")\n",
    "\n",
    "print(xv.shape)  # (6200, 1024)\n",
    "\n",
    "YPredicted = model_py.predict(xv, verbose=1, batch_size=128)\n",
    "\n",
    "print(YPredicted.shape)\n",
    "\n",
    "Indmax_DL_py = np.argmax(YPredicted, axis=1)\n",
    "print(Indmax_DL_py.shape)\n",
    "print(np.min(Indmax_DL_py))\n",
    "print(np.max(Indmax_DL_py))\n",
    "\n",
    "# Questi devono essere numeri interi\n",
    "\n",
    "Indmax_DL = Indmax_DL_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2450f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(MaxR_DL): (6200,)\n",
      "Rate_DL: 1.0489205121994019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inizializzazione\n",
    "#validation_accuracy = 0\n",
    "MaxR_DL = np.zeros((Indmax_DL.shape[0],), dtype=np.float32)\n",
    "#MaxR_OPT = np.zeros((len(Indmax_OPT),), dtype=np.float32)\n",
    "\n",
    "# Ciclo di confronto\n",
    "for b in range(Indmax_DL.shape[0]):\n",
    "    #MaxR_DL[b] = YValidation_un[b, Indmax_DL[b], 0, 0]\n",
    "    MaxR_DL[b] = YValidation_un2[0, 0, Indmax_DL[b], b]\n",
    "    #MaxR_OPT[b] = YValidation_un[b, Indmax_OPT[b], 0, 0]\n",
    "\n",
    "    #if MaxR_DL[b] == MaxR_OPT[b]:\n",
    "    #    validation_accuracy += 1\n",
    "\n",
    "\n",
    "Rate_DL = MaxR_DL.mean()\n",
    "#Rate_OPT = MaxR_OPT.mean()\n",
    "#validation_accuracy = validation_accuracy / Indmax_DL.shape[0]\n",
    "\n",
    "# Output finali\n",
    "print(f\"size(MaxR_DL): {MaxR_DL.shape}\")\n",
    "#print(f\"size(MaxR_OPT): {MaxR_OPT.shape}\")\n",
    "#print(f\"Rate_OPT: {Rate_OPT}\")\n",
    "print(f\"Rate_DL: {Rate_DL}\") # deve venire circa 1 per 10000 con 20 epoche\n",
    "#print(f\"validation_accuracy: {validation_accuracy}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f4c699c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "20 epoche matlab: Rate_DL: 1.0489205121994019\n",
    "20 epoche:        Rate_DL: 1.5136982202529907\n",
    "40 epoche:        Rate_DL: 1.5716270208358765\n",
    "                 Rate_OPT: 1.7807620763778687"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "921d96e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate_OPT: 1.7807620763778687\n"
     ]
    }
   ],
   "source": [
    "filename_Rate_OPT = (\n",
    "    f\"{network_folder_in}Rate_OPT\"\n",
    "    f\"_seed{seed}\"\n",
    "    f\"_grid{Ur_rows[1]}\"  # Ur_rows(2) in MATLAB diventa Ur_rows[1] in Python\n",
    "    f\"_M{My}{Mz}\"\n",
    "    f\"_Mbar{M_bar}\"\n",
    "    f\"_{Training_Size_dd}.mat\"\n",
    ")\n",
    "\n",
    "with h5py.File(filename_Rate_OPT, 'r') as f:\n",
    "    Rate_OPT = f['Rate_OPT'][:][0][0]\n",
    "\n",
    "print(f\"Rate_OPT: {Rate_OPT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418854b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b1c10f",
   "metadata": {},
   "source": [
    "### Test: import Matlab weights on Python model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_traininfo = (\n",
    "    f\"{network_folder_in}traininfo\"\n",
    "    f\"_seed{seed}\"\n",
    "    f\"_grid{Ur_rows[1]}\"\n",
    "    f\"_M{My}{Mz}\"\n",
    "    f\"_Mbar{M_bar}\"\n",
    "    f\"_{Training_Size_dd}.mat\"\n",
    ")\n",
    "\n",
    "def read_mat_struct_field(file, struct_path, field):\n",
    "    \"\"\"Legge un campo da una struct MATLAB v7.3 (HDF5)\"\"\"\n",
    "    ref = file[struct_path][field][0, 0]\n",
    "    return ref\n",
    "\n",
    "with h5py.File(filename_traininfo, 'r') as f:\n",
    "    # Visualizza tutti i gruppi disponibili nella root del file\n",
    "    print(\"Gruppi principali nel file:\")\n",
    "    for group in f.keys():\n",
    "        print(\"-\", group)\n",
    "    \n",
    "    # Visualizza le chiavi di 'traininfo'\n",
    "    print(\"\\nCampi disponibili in 'traininfo':\")\n",
    "    for field in f['traininfo'].keys():\n",
    "        print(\"-\", field)\n",
    "\n",
    "    \n",
    "    FinalValidationLoss = f['traininfo']['FinalValidationLoss'][0, 0]\n",
    "    FinalValidationRMSE = f['traininfo']['FinalValidationRMSE'][0, 0]\n",
    "    TrainingLoss = f['traininfo']['TrainingLoss'][-1, 0]\n",
    "    TrainingRMSE = f['traininfo']['TrainingRMSE'][-1, 0]\n",
    "    ValidationLoss = f['traininfo']['ValidationLoss'][-1, 0]\n",
    "    ValidationRMSE = f['traininfo']['ValidationRMSE'][-1, 0]\n",
    "    #OutputNetworkIteration = f['traininfo']['OutputNetworkIteration'][0, 0]\n",
    "\n",
    "    # è un numero senza unità e serve principalmente come obiettivo di ottimizzazione.\n",
    "    # è una misura dell'errore medio delle predizioni rispetto ai veri valori\n",
    "    print(\"\\nTrainingLoss:\", TrainingLoss) \n",
    "    print(\"ValidationLoss:\", ValidationLoss)\n",
    "    print(\"FinalValidationLoss:\", FinalValidationLoss) #  il valore finale della funzione di perdita (loss, MSE) calcolata sui dati di validazione,\n",
    "    print(\"\\nTrainingRMSE:\", TrainingRMSE)\n",
    "    print(\"ValidationRMSE:\", ValidationRMSE)\n",
    "    print(\"FinalValidationRMSE:\", FinalValidationRMSE) # il valore finale del Root Mean Squared Error (RMSE) sui dati di validazione,\n",
    "    #print(\"OutputNetworkIteration:\", OutputNetworkIteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e160a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "\n",
    "# Definizione del nuovo input (non normalizzato)\n",
    "#input_raw = Input(shape=(1, 1, X_train.shape[1]), name='input_unnormalized')\n",
    "input_raw = Input(shape=(X_train.shape[1],), name='input_unnormalized')\n",
    "\n",
    "# Ricostruzione della rete, escluso il layer 'input_'\n",
    "#x = Reshape((1, 1, 1024), name='Fully1_preFlatten1')(input_raw)\n",
    "x = Dense(1024, name='Fully1_')(input_raw)\n",
    "x = ReLU(name='re_lu')(x)\n",
    "x = Dropout(0.5, name='dropout')(x)\n",
    "\n",
    "#x = Reshape((1, 1, 1024), name='Fully2_preFlatten1')(x)\n",
    "x = Dense(4096, name='Fully2_')(x)\n",
    "x = ReLU(name='re_lu_1')(x)\n",
    "x = Dropout(0.5, name='dropout_1')(x)\n",
    "\n",
    "#x = Reshape((1, 1, 4096), name='Fully3_preFlatten1')(x)\n",
    "x = Dense(4096, name='Fully3_')(x)\n",
    "x = ReLU(name='re_lu_2')(x)\n",
    "x = Dropout(0.5, name='dropout_2')(x)\n",
    "\n",
    "#x = Reshape((1, 1, 4096), name='Fully4_preFlatten1')(x)\n",
    "#x = Dense(1024, name='Fully4_')(x)\n",
    "#output = Flatten(name='flatten')(x)\n",
    "output = Dense(1024, name='Fully4_')(x)\n",
    "\n",
    "# Costruzione del nuovo modello\n",
    "model_mat_no_norm = Model(inputs=input_raw, outputs=output, name='model_mat_no_norm')\n",
    "\n",
    "# Copia dei pesi dai layer omologhi di model_mat\n",
    "for layer in model_py.layers:\n",
    "    try:\n",
    "        corresponding_layer = model_mat_no_norm.get_layer(layer.name)\n",
    "        layer.set_weights(corresponding_layer.get_weights())\n",
    "        print(f\"Pesi copiati per il layer: {layer.name}\")\n",
    "    except ValueError:\n",
    "        print(f\"Layer '{layer.name}' non trovato in model_mat o senza pesi.\")\n",
    "\n",
    "model_py.summary()\n",
    "\n",
    "# Sanity check\n",
    "\n",
    "#print(xv.shape)  # (6200, 1, 1, 1024)\n",
    "\n",
    "#X_val_reshaped = tf.transpose(X_val, perm=[0, 3, 1, 2]) \n",
    "#print(X_val_reshaped.shape)  # (6200, 1024, 1, 1)\n",
    "\n",
    "# Esegui predizione con DL_input_reshaped\n",
    "#YPredicted_mat = model_mat_no_norm.predict(X_val_normalized, batch_size=128, verbose=1)\n",
    "YPredicted_mat = model_mat_no_norm.predict(X_val, batch_size=128, verbose=1)\n",
    "print(YPredicted_mat.shape)\n",
    "\n",
    "Indmax_DL_mat_no_norm = np.argmax(YPredicted_mat, axis=1)\n",
    "print(Indmax_DL_mat_no_norm.shape)\n",
    "print(np.min(Indmax_DL_mat_no_norm))\n",
    "print(np.max(Indmax_DL_mat_no_norm))\n",
    "\n",
    "Indmax_DL = Indmax_DL_mat_no_norm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepmimo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
