{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e7c56c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:50:19.218790: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745502619.237608  164751 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745502619.243009  164751 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745502619.258064  164751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745502619.258086  164751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745502619.258088  164751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745502619.258090  164751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "#import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#import absl.logging\n",
    "#absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Sopprime INFO, WARNING ed ERROR\n",
    "\n",
    "#tf.get_logger().setLevel('ERROR')  # Nasconde i messaggi di logging di TensorFlow\n",
    "#logging.disable(logging.WARNING)\n",
    "#logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "#sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Imposta il seed globale\n",
    "seed = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "#from tensorflow.keras.constraints import MinMaxNorm\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.saving import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Normalization, Dense, Dropout, ReLU, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, TensorBoard, ProgbarLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "import sys\n",
    "import importlib\n",
    "import time\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import psutil \n",
    "\n",
    "# Imposta float32 come tipo predefinito in Keras\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "# Tipo da usare esplicitamente (es. in numpy)\n",
    "force_datatype = np.float32\n",
    "\n",
    "use_gpu = 1\n",
    "\n",
    "# ------------------ Configurazione GPU ------------------ #\n",
    "if not use_gpu:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disabilita la GPU\n",
    "\n",
    "print(f\"\\nUsing GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "tf.config.optimizer.set_jit(False)  # XLA off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa65ba9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TensorBoard avviato in background.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:58:23.262217: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745503103.280323  169789 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745503103.285603  169789 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745503103.299788  169789 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745503103.299818  169789 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745503103.299822  169789 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745503103.299824  169789 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "log_dir = \"/mnt/c/Users/Work/Desktop/deepMIMO/RIS/DeepMIMOv1-LIS-DeepLearning-Taha/Output_Python/Neural_Network/tensorboard_logs_test/\"\n",
    "tensorboard_command = [\n",
    "    \"tensorboard\",\n",
    "    f\"--logdir={log_dir}\",\n",
    "    \"--port=6006\",\n",
    "    \"--host=localhost\"\n",
    "]\n",
    "\n",
    "# Funzione per trovare e terminare TensorBoard se è già in esecuzione\n",
    "def terminate_tensorboard():\n",
    "    for process in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "        try:\n",
    "            if 'tensorboard' in process.info['name'] or \\\n",
    "               (process.info['cmdline'] and 'tensorboard' in process.info['cmdline'][0]):\n",
    "                print(f\"\\nTerminazione di TensorBoard con PID: {process.info['pid']}\")\n",
    "                os.kill(process.info['pid'], signal.SIGTERM)\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
    "            pass\n",
    "\n",
    "# Controlla se TensorBoard è già in esecuzione\n",
    "try:\n",
    "    # Avvia TensorBoard in background\n",
    "    tensorboard_process = subprocess.Popen(tensorboard_command)\n",
    "    print(f\"\\nTensorBoard avviato in background.\")\n",
    "except:\n",
    "    print(f\"\\nfErrore nell'avvio di TensorBoard: {e}\")\n",
    "    # Chiudi TensorBoard se è già in esecuzione\n",
    "    terminate_tensorboard()\n",
    "    # Avvia TensorBoard in background\n",
    "    tensorboard_process = subprocess.Popen(tensorboard_command)\n",
    "    print(f\"\\nTensorBoard avviato in background.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caa35922",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Define functions"
   },
   "outputs": [],
   "source": [
    "\n",
    "def model_predict(xtest, model_py, YValidation_un2):\n",
    "\n",
    "    print(f\"\\nStart DL prediction...\")\n",
    "\n",
    "    #print(xtest.shape)  # (6200, 1024)\n",
    "\n",
    "    YPredicted = model_py.predict(xtest, verbose=1, batch_size=128)\n",
    "\n",
    "    #print(YPredicted.shape)\n",
    "\n",
    "    Indmax_DL_py = np.argmax(YPredicted, axis=1)\n",
    "    #print(Indmax_DL_py.shape)\n",
    "    print(np.min(Indmax_DL_py))\n",
    "    print(np.max(Indmax_DL_py))\n",
    "\n",
    "    # Questi devono essere numeri interi\n",
    "\n",
    "    Indmax_DL = Indmax_DL_py\n",
    "\n",
    "    #validation_accuracy = 0\n",
    "    MaxR_DL = np.zeros((Indmax_DL.shape[0],), dtype=np.float32)\n",
    "    #MaxR_OPT = np.zeros((len(Indmax_OPT),), dtype=np.float32)\n",
    "\n",
    "    # Ciclo di confronto\n",
    "    for b in range(Indmax_DL.shape[0]):\n",
    "        #MaxR_DL[b] = YValidation_un[b, Indmax_DL[b], 0, 0]\n",
    "        MaxR_DL[b] = YValidation_un2[0, 0, Indmax_DL[b], b]\n",
    "        #MaxR_OPT[b] = YValidation_un[b, Indmax_OPT[b], 0, 0]\n",
    "\n",
    "        #if MaxR_DL[b] == MaxR_OPT[b]:\n",
    "        #    validation_accuracy += 1\n",
    "\n",
    "    Rate_DL_py = MaxR_DL.mean()\n",
    "    #Rate_OPT = MaxR_OPT.mean()\n",
    "    #validation_accuracy = validation_accuracy / Indmax_DL.shape[0]\n",
    "\n",
    "    #print(f\"size(MaxR_DL): {MaxR_DL.shape}\")\n",
    "\n",
    "    return Rate_DL_py\n",
    "\n",
    "def mse_keras(y_true, y_pred): # Not working\n",
    "    squared_error = tf.square(y_true - y_pred)  # shape: (batch_size, output_dim)=6200,1024\n",
    "    loss = tf.reduce_mean(squared_error) # scalar\n",
    "    return loss\n",
    "\n",
    "def mse_custom(y_true, y_pred):\n",
    "    # Calcola l'errore quadratico tra vero e predetto\n",
    "    squared_error = tf.square(y_true - y_pred)  # shape: (batch_size, output_dim)=6200,1024\n",
    "\n",
    "    # Somma degli errori lungo l'ultima dimensione (output_dim)\n",
    "    sum_squared_error = tf.reduce_sum(squared_error, axis=-1)  # shape: (batch_size,)=6200\n",
    "\n",
    "    # Media su tutto il batch\n",
    "    loss = 0.5 * tf.reduce_mean(sum_squared_error)  # scalar\n",
    "    return loss\n",
    "\n",
    "def mse_matlab(y_true, y_pred): # Not working\n",
    "    # A regression layer computes the half-mean-squared-error loss for regression tasks:\n",
    "    # https://www.mathworks.com/help//releases/R2021a/deeplearning/ref/regressionlayer.html?searchHighlight=regressionLayer&searchResultIndex=1\n",
    "    squared_error = tf.square(y_true - y_pred)  # shape: (batch_size, output_dim)=6200,1024\n",
    "    \n",
    "    loss = 0.5 * tf.reduce_mean(squared_error)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268730fa",
   "metadata": {},
   "source": [
    "## Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fadfd186",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = '/mnt/c/Users/Work/Desktop/deepMIMO/RIS/DeepMIMOv1-LIS-DeepLearning-Taha/'\n",
    "\n",
    "input_folder = base_folder + 'Output Matlab/'\n",
    "\n",
    "#DeepMIMO_dataset_folder = input_folder + 'DeepMIMO Dataset/'\n",
    "DL_dataset_folder = input_folder + 'DL Dataset/'\n",
    "network_folder_in = input_folder + 'Neural Network/'\n",
    "\n",
    "output_folder = base_folder + 'Output_Python/'\n",
    "network_folder_out = output_folder + 'Neural_Network/'\n",
    "network_folder_out_YPredicted = output_folder + 'Neural_Network/YPredicted/'\n",
    "network_folder_out_RateDLpy = output_folder + 'Neural_Network/RateDLpy/'\n",
    "saved_models = network_folder_out + 'saved_models/'\n",
    "figure_folder = output_folder + 'Figures/'\n",
    "\n",
    "import os\n",
    "\n",
    "folders = [\n",
    "    output_folder,\n",
    "    network_folder_out,\n",
    "    network_folder_out_YPredicted,\n",
    "    network_folder_out_RateDLpy,\n",
    "    saved_models,\n",
    "    figure_folder\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):  # Controlla se la cartella esiste\n",
    "        os.makedirs(folder, exist_ok=True)  # Crea la cartella se non esiste\n",
    "        print(f\"\\nCartella creata: {folder}\")\n",
    "    #else:\n",
    "    #    print(f\"La cartella esiste già: {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8463c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "My_ar = [32, 64]\n",
    "Mz_ar = [32, 64]\n",
    "My_ar = [32]\n",
    "Mz_ar = [32]\n",
    "#My_ar = [64]\n",
    "#Mz_ar = [64]\n",
    "Mx = 1\n",
    "\n",
    "M_bar=8\n",
    "Ur_rows = [1000, 1200]\n",
    "#              0    1      2      3      4      5      6\n",
    "Training_Size=[2, 10000, 14000, 18000, 22000, 26000, 30000]\n",
    "#Training_Size=[10000, 14000, 18000, 22000, 26000, 30000]\n",
    "#Training_Size=[10000]\n",
    "\n",
    "\n",
    "load_model_flag = 0\n",
    "max_epochs_load = 20\n",
    "\n",
    "train_model_flag = 1\n",
    "max_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba471c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RIS: 32x32\n",
      "\n",
      "Training_Size_dd: 10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36200, 1, 1, 1024)\n",
      "(36200, 1024, 1, 1)\n",
      "(1, 36200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# count, value\n",
    "#for i, ris in enumerate(My_ar):\n",
    "ris = 32\n",
    "\n",
    "#My = My_ar[i]\n",
    "#Mz = Mz_ar[i]\n",
    "My = ris\n",
    "Mz = ris\n",
    "print(f\"\\nRIS: {My}x{Mz}\")\n",
    "\n",
    "#for j, Training_Size_dd in enumerate(Training_Size):\n",
    "Training_Size_dd = Training_Size[1]\n",
    "\n",
    "end_folder = '_seed' + str(seed) + '_grid' + str(Ur_rows[1]) + '_M' + str(My) + str(Mz) + '_Mbar' + str(M_bar)\n",
    "end_folder_Training_Size_dd = end_folder + '_' + str(Training_Size_dd)\n",
    "\n",
    "print(f\"\\nTraining_Size_dd: {Training_Size_dd}\")\n",
    "\n",
    "Rate_DL_py = 0\n",
    "    \n",
    "## Load Dataset DL_input_reshaped\n",
    "\n",
    "filename_DL_input_reshaped = DL_dataset_folder + 'DL_input_reshaped' + end_folder + '.mat'\n",
    "filename_DL_output_reshaped = DL_dataset_folder + 'DL_output_reshaped' + end_folder + '.mat'\n",
    "filename_RandP_all = DL_dataset_folder + 'RandP_all' + end_folder + '.mat'\n",
    "\n",
    "# Load the data using h5py for MATLAB v7.3 files\n",
    "with h5py.File(filename_DL_input_reshaped, 'r') as f:\n",
    "    DL_input_reshaped = np.array(f['DL_input_reshaped'][:], dtype=force_datatype)\n",
    "with h5py.File(filename_DL_output_reshaped, 'r') as f:\n",
    "    DL_output_reshaped = np.array(f['DL_output_reshaped'][:], dtype=force_datatype)\n",
    "with h5py.File(filename_RandP_all, 'r') as f:\n",
    "    RandP_all = np.array(f['RandP_all'][:], dtype=force_datatype)\n",
    "\n",
    "print(DL_input_reshaped.shape)\n",
    "print(DL_output_reshaped.shape)\n",
    "print(RandP_all.shape)\n",
    "\n",
    "#print(np.min(DL_input_reshaped))\n",
    "#print(np.max(DL_input_reshaped))\n",
    "#print(np.min(DL_output_reshaped))\n",
    "#print(np.max(DL_output_reshaped))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5732fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Rates\n",
    "\n",
    "# Costruzione del nome file\n",
    "filename_DL_output_un_reshaped = DL_dataset_folder + 'DL_output_un_reshaped' + end_folder + '.mat'\n",
    "\n",
    "# Load the data using h5py for MATLAB v7.3 files\n",
    "with h5py.File(filename_DL_output_un_reshaped, 'r') as f:\n",
    "    # Accesso alla variabile (nome del dataset = nome della variabile in MATLAB)\n",
    "    YValidation_un = np.array(f['DL_output_un_reshaped'], dtype=force_datatype)\n",
    "\n",
    "#print(YValidation_un.shape)\n",
    "\n",
    "YValidation_un2 = np.transpose(YValidation_un, (3, 2, 1, 0))  # conversione a (b, z, y, x)\n",
    "#print(YValidation_un2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba662761",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Dataset split originale\n",
    "\n",
    "# Flatten the input and output arrays if necessary\n",
    "#X = DL_input_reshaped.reshape(DL_input_reshaped.shape[0], -1).astype(np.float32)\n",
    "#Y = DL_output_reshaped.reshape(DL_output_reshaped.shape[0], -1).astype(np.float32)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "#X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=validation_size / (training_size + validation_size), shuffle=False, random_state=seed)\n",
    "\n",
    "RandP_all2 = np.squeeze(np.array(RandP_all.astype(int))) - 1\n",
    "\n",
    "Training_Ind = RandP_all2[0:Training_Size_dd]\n",
    "\n",
    "Validation_Size = 6200\n",
    "Validation_Ind = RandP_all2[-Validation_Size:]\n",
    "\n",
    "#print(Training_Ind.shape)\n",
    "#print(Validation_Ind.shape)\n",
    "\n",
    "X_train = np.array(DL_input_reshaped[Training_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "Y_train = np.array(DL_output_reshaped[Training_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "X_val = np.array(DL_input_reshaped[Validation_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "Y_val = np.array(DL_output_reshaped[Validation_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "\n",
    "print(f\"\\nY_train.dtype: {Y_train.dtype}\")\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(Y_train.shape)\n",
    "#print(X_val.shape)\n",
    "#print(Y_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5e4ebb3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Dataset split nuovo con test set, ipotesi di split 80-20\n",
    "\n",
    "dataset_size = 36200\n",
    "\n",
    "perc = 0.8\n",
    "Dev_Size = round(perc * dataset_size)\n",
    "Test_Size = round((1-perc) * dataset_size)\n",
    "Validation_Size = Test_Size\n",
    "Train_Size = Dev_Size - Validation_Size\n",
    "\n",
    "print(Dev_Size)\n",
    "print(Train_Size)\n",
    "print(Validation_Size)\n",
    "print(Test_Size)\n",
    "\n",
    "perc_train = Train_Size/Dev_Size\n",
    "perc_val = Validation_Size/Dev_Size\n",
    "print(perc_train)\n",
    "print(perc_val)\n",
    "\n",
    "RandP_all2 = np.squeeze(np.array(RandP_all.astype(int))) - 1\n",
    "\n",
    "Training_Ind = RandP_all2[0:Training_Size_dd]\n",
    "Test_Ind = RandP_all2[-Test_Size:]\n",
    "Validation_Ind = RandP_all2[-(Validation_Size+Test_Size):-Test_Size]\n",
    "\n",
    "print(Training_Ind.shape)\n",
    "print(Validation_Ind.shape)\n",
    "print(Test_Ind.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62dda5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "26900\n",
      "3100\n",
      "3100\n",
      "0.8966666666666666\n",
      "0.10333333333333333\n",
      "(10000,)\n",
      "(3100,)\n",
      "(3100,)\n"
     ]
    }
   ],
   "source": [
    "## Dataset split nuovo con test set, ipotesi di split 90-10 (per avere val+test = Validazion_Size = 6200)\n",
    "# L'importante è garantire un sufficiente numero di sample di test che rappresenti bene la distribuzione dei dati\n",
    "\n",
    "dataset_size = 36200\n",
    "\n",
    "Validation_Size_old = 6200\n",
    "Test_Size = int(Validation_Size_old/2)\n",
    "Validation_Size = Test_Size\n",
    "Dev_Size = dataset_size - Validation_Size - Test_Size\n",
    "Train_Size = Dev_Size - Validation_Size\n",
    "\n",
    "print(Dev_Size)\n",
    "print(Train_Size)\n",
    "print(Validation_Size)\n",
    "print(Test_Size)\n",
    "\n",
    "perc_train = Train_Size/Dev_Size\n",
    "perc_val = Validation_Size/Dev_Size\n",
    "print(perc_train)\n",
    "print(perc_val)\n",
    "\n",
    "RandP_all2 = np.squeeze(np.array(RandP_all.astype(int))) - 1\n",
    "\n",
    "Training_Ind = RandP_all2[0:Training_Size_dd]\n",
    "Test_Ind = RandP_all2[-Test_Size:]\n",
    "Validation_Ind = RandP_all2[-(Validation_Size+Test_Size):-Test_Size]\n",
    "\n",
    "print(Training_Ind.shape)\n",
    "print(Validation_Ind.shape)\n",
    "print(Test_Ind.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1e77f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Y_train.dtype: float32\n",
      "(10000, 1024)\n",
      "(10000, 1024)\n",
      "(3100, 1024)\n",
      "(3100, 1024)\n",
      "(3100, 1024)\n",
      "(3100, 1024)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(DL_input_reshaped[Training_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "Y_train = np.array(DL_output_reshaped[Training_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "X_val   = np.array(DL_input_reshaped[Validation_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "Y_val   = np.array(DL_output_reshaped[Validation_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "X_test  = np.array(DL_input_reshaped[Test_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "Y_test  = np.array(DL_output_reshaped[Test_Ind, :, :, :], dtype=force_datatype).squeeze()\n",
    "\n",
    "print(f\"\\nY_train.dtype: {Y_train.dtype}\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f57c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trainedNet_scaler: 0.00030796974897384644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745502705.679391  164751 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13709 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:47:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Fully1_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully2_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully3_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">16,781,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully4_ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Fully1_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m1,049,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu1 (\u001b[38;5;33mReLU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully2_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │     \u001b[38;5;34m4,198,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu2 (\u001b[38;5;33mReLU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully3_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │    \u001b[38;5;34m16,781,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu3 (\u001b[38;5;33mReLU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Fully4_ (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,224,640</span> (100.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,224,640\u001b[0m (100.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,224,640</span> (100.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,224,640\u001b[0m (100.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Recreate the same network in Python\n",
    "\n",
    "# ### Load normalization parameters from Matlab trained model\n",
    "\n",
    "filename_trainedNet_scaler = network_folder_in + 'trainedNet_scaler' + end_folder_Training_Size_dd + '.mat'\n",
    "\n",
    "with h5py.File(filename_trainedNet_scaler, 'r') as f:\n",
    "    trainedNet_scaler = f['trainedNet_scaler'][:][0][0]\n",
    "\n",
    "#print(trainedNet_scaler.shape)\n",
    "print(f\"\\ntrainedNet_scaler: {trainedNet_scaler}\") # should be -5.1904644e-06 for Training_Size_dd=30000\n",
    "\n",
    "\n",
    "# This layer will shift and scale inputs into a distribution centered around 0 with standard deviation 1. \n",
    "# It accomplishes this by precomputing the mean and variance of the data, and calling (input - mean) / sqrt(var) at runtime.\n",
    "# The mean and variance values for the layer must be either supplied on construction or learned via adapt(). \n",
    "# adapt() will compute the mean and variance of the data and store them as the layer's weights. \n",
    "# adapt() should be called before fit(), evaluate(), or predict().\n",
    "\n",
    "mean_array = np.array([trainedNet_scaler]*X_train.shape[1], dtype=force_datatype)\n",
    "variance_array =  np.array([1]*X_train.shape[1], dtype=force_datatype)\n",
    "#print(mean_array.shape)\n",
    "#print(mean_array[0])\n",
    "#print(variance_array.shape)\n",
    "#print(variance_array[0])\n",
    "\n",
    "### Normalize data\n",
    "# Normalizzazione manuale se già hai mean_array e variance_array da MATLAB\n",
    "X_train_normalized = np.array((X_train - mean_array) / np.sqrt(variance_array), dtype=force_datatype)\n",
    "X_val_normalized = np.array((X_val - mean_array) / np.sqrt(variance_array), dtype=force_datatype)\n",
    "X_test_normalized = np.array((X_test - mean_array) / np.sqrt(variance_array), dtype=force_datatype)\n",
    "\n",
    "#print(mean_array)\n",
    "#print(variance_array)\n",
    "\n",
    "normalized = 1\n",
    "\n",
    "#print(X_train[0][0:5])\n",
    "#print(mean_array[0:5])\n",
    "#print(X_train_normalized[0][0:5])\n",
    "\n",
    "## DL Model Definition\n",
    "\n",
    "if normalized == 1:\n",
    "    xtrain = X_train_normalized\n",
    "    xval = X_val_normalized\n",
    "    xtest = X_val_normalized\n",
    "else:\n",
    "    xtrain = X_train\n",
    "    xval = X_val\n",
    "    xtest = X_test\n",
    "\n",
    "# Define the neural network architecture\n",
    "model_py = Sequential([\n",
    "    Input(shape=(X_train.shape[1],), name='input'),\n",
    "\n",
    "    Dense(units=Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully1_'),\n",
    "    ReLU(name='relu1'),\n",
    "    Dropout(0.5, name='dropout1'),\n",
    "\n",
    "    Dense(units=4 * Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully2_'),\n",
    "    ReLU(name='relu2'),\n",
    "    Dropout(0.5, name='dropout2'),\n",
    "\n",
    "    Dense(units=4 * Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully3_'),\n",
    "    ReLU(name='relu3'),\n",
    "    Dropout(0.5, name='dropout3'),\n",
    "\n",
    "    Dense(units=Y_train.shape[1], kernel_regularizer=l2(1e-4), name='Fully4_'),\n",
    "])\n",
    "\n",
    "# Compile the model with SGD optimizer and mean squared error loss\n",
    "optimizer = SGD(learning_rate=1e-1, momentum=0.9)\n",
    "\n",
    "#model_py.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
    "#model_py.compile(optimizer=optimizer, loss=mse_keras, metrics=['mse'])\n",
    "#model_py.compile(optimizer=optimizer, loss=mse_matlab, metrics=['mse'])\n",
    "model_py.compile(optimizer=optimizer, loss=mse_custom, metrics=['mse'])\n",
    "model_py.summary()\n",
    "\n",
    "#print(model_py.loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cbaf4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start DL training...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745502760.800289  165183 service.cc:152] XLA service 0x7f846c004dd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745502760.800327  165183 service.cc:160]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "I0000 00:00:1745502760.971234  165183 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1745502766.321172  165183 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 17s - 842ms/step - loss: 7.2782 - mse: 0.0125 - val_loss: 5.3014 - val_mse: 0.0085 - learning_rate: 0.1000\n",
      "Epoch 2/20\n",
      "20/20 - 1s - 66ms/step - loss: 5.1647 - mse: 0.0082 - val_loss: 5.0415 - val_mse: 0.0079 - learning_rate: 0.1000\n",
      "Epoch 3/20\n",
      "20/20 - 1s - 67ms/step - loss: 5.0373 - mse: 0.0079 - val_loss: 5.0044 - val_mse: 0.0079 - learning_rate: 0.1000\n",
      "Epoch 4/20\n",
      "20/20 - 1s - 57ms/step - loss: 4.9928 - mse: 0.0079 - val_loss: 4.8577 - val_mse: 0.0076 - learning_rate: 0.1000\n",
      "Epoch 5/20\n",
      "20/20 - 2s - 83ms/step - loss: 4.8835 - mse: 0.0077 - val_loss: 4.6159 - val_mse: 0.0071 - learning_rate: 0.1000\n",
      "Epoch 6/20\n",
      "20/20 - 1s - 55ms/step - loss: 4.6709 - mse: 0.0073 - val_loss: 4.3191 - val_mse: 0.0066 - learning_rate: 0.1000\n",
      "Epoch 7/20\n",
      "20/20 - 1s - 27ms/step - loss: 4.3833 - mse: 0.0067 - val_loss: 3.8785 - val_mse: 0.0057 - learning_rate: 0.1000\n",
      "Epoch 8/20\n",
      "20/20 - 1s - 56ms/step - loss: 4.1452 - mse: 0.0063 - val_loss: 3.4964 - val_mse: 0.0050 - learning_rate: 0.1000\n",
      "Epoch 9/20\n",
      "20/20 - 1s - 55ms/step - loss: 3.9440 - mse: 0.0059 - val_loss: 3.4130 - val_mse: 0.0048 - learning_rate: 0.1000\n",
      "Epoch 10/20\n",
      "20/20 - 1s - 58ms/step - loss: 3.7853 - mse: 0.0056 - val_loss: 3.1663 - val_mse: 0.0044 - learning_rate: 0.1000\n",
      "Epoch 11/20\n",
      "20/20 - 1s - 56ms/step - loss: 3.5866 - mse: 0.0052 - val_loss: 3.0067 - val_mse: 0.0041 - learning_rate: 0.1000\n",
      "Epoch 12/20\n",
      "20/20 - 1s - 64ms/step - loss: 3.4174 - mse: 0.0049 - val_loss: 2.8417 - val_mse: 0.0038 - learning_rate: 0.1000\n",
      "Epoch 13/20\n",
      "20/20 - 1s - 56ms/step - loss: 3.3122 - mse: 0.0047 - val_loss: 2.7346 - val_mse: 0.0036 - learning_rate: 0.1000\n",
      "Epoch 14/20\n",
      "20/20 - 1s - 65ms/step - loss: 3.1966 - mse: 0.0045 - val_loss: 2.6676 - val_mse: 0.0034 - learning_rate: 0.1000\n",
      "Epoch 15/20\n",
      "20/20 - 1s - 58ms/step - loss: 3.1016 - mse: 0.0043 - val_loss: 2.5439 - val_mse: 0.0032 - learning_rate: 0.1000\n",
      "Epoch 16/20\n",
      "20/20 - 2s - 85ms/step - loss: 3.0227 - mse: 0.0042 - val_loss: 2.4986 - val_mse: 0.0031 - learning_rate: 0.1000\n",
      "Epoch 17/20\n",
      "20/20 - 1s - 56ms/step - loss: 2.9241 - mse: 0.0040 - val_loss: 2.4097 - val_mse: 0.0030 - learning_rate: 0.1000\n",
      "Epoch 18/20\n",
      "20/20 - 2s - 79ms/step - loss: 2.8740 - mse: 0.0039 - val_loss: 2.3728 - val_mse: 0.0029 - learning_rate: 0.1000\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "20/20 - 1s - 57ms/step - loss: 2.8216 - mse: 0.0038 - val_loss: 2.3336 - val_mse: 0.0029 - learning_rate: 0.1000\n",
      "Epoch 20/20\n",
      "20/20 - 1s - 56ms/step - loss: 2.7793 - mse: 0.0037 - val_loss: 2.2755 - val_mse: 0.0027 - learning_rate: 0.0500\n",
      "Training completed in 0.68 minutes.\n",
      "\n",
      "Model saved in /mnt/c/Users/Work/Desktop/deepMIMO/RIS/DeepMIMOv1-LIS-DeepLearning-Taha/Output_Python/Neural_Network/saved_models/\n"
     ]
    }
   ],
   "source": [
    "## DL Model Training\n",
    "\n",
    "# ------------------ Training Options ------------------ #\n",
    "mini_batch_size = 500\n",
    "\n",
    "if load_model_flag == 1:\n",
    "    max_epochs_new = max_epochs_load + max_epochs\n",
    "    factor = 0.5\n",
    "    patience = 3\n",
    "    min_delta = 0.05\n",
    "else:\n",
    "    max_epochs_new = max_epochs\n",
    "    factor = 0.5\n",
    "    patience = 2\n",
    "    min_delta = 0.1\n",
    "\n",
    "# For the output filenames\n",
    "end_folder_Training_Size_dd_max_epochs = end_folder_Training_Size_dd + '_' + str(max_epochs_new)\n",
    "filename_Rate_DL_py = network_folder_out_RateDLpy + 'Rate_DL_py' + end_folder_Training_Size_dd_max_epochs + '.mat'\n",
    "model_type = 'model_py_test' + end_folder_Training_Size_dd_max_epochs\n",
    "\n",
    "tensorboard_logs = log_dir + model_type\n",
    "tensorboard_callback = TensorBoard(log_dir=tensorboard_logs, histogram_freq=1)\n",
    "\n",
    "#if Training_Size_dd < mini_batch_size:\n",
    "#    validationFrequency = Training_Size_dd\n",
    "#else:\n",
    "#    validationFrequency = int(np.floor(Training_Size_dd/mini_batch_size))\n",
    "validationFrequency = 1\n",
    "\n",
    "# ------------------ Learning Rate Scheduler ------------------ #\n",
    "#def lr_schedule(epoch, lr):\n",
    "#    if epoch > 0 and epoch % 5 == 0: # Prima era modulo 3\n",
    "#        return lr * 0.5  # Drop learning rate by factor of 0.5 every x epochs\n",
    "#    return lr\n",
    "\n",
    "#lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=factor,         # Riduce di metà\n",
    "    patience=patience,  # Numero di epoche senza miglioramento ≥ y\n",
    "    min_delta=min_delta,      # Miglioramento minimo da considerare significativo\n",
    "    verbose=1\n",
    ")\n",
    "        \n",
    "# ------------------ DL Model Training ------------------ #\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices((x, Y_train)).batch(mini_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "#val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(mini_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "if load_model_flag == 1:\n",
    "    end_folder_Training_Size_dd_max_epochs_load = end_folder_Training_Size_dd + '_' + str(max_epochs_load)\n",
    "    model_type_load = 'model_py_test' + end_folder_Training_Size_dd_max_epochs_load\n",
    "\n",
    "    model_py = load_model(saved_models + model_type_load + '.keras', custom_objects={'mse_custom': mse_custom})\n",
    "    print(f\"\\nModel {saved_models + model_type_load + '.keras'} loaded\")\n",
    "\n",
    "    Rate_DL_py_load = model_predict(xtest, model_py, YValidation_un2)\n",
    "    print(f\"Rate_DL_py: {Rate_DL_py_load}\")\n",
    "\n",
    "    learning_rate = model_py.optimizer.learning_rate.numpy()\n",
    "    print(f\"Learning rate loaded model: {learning_rate}\")\n",
    "\n",
    "\n",
    "if train_model_flag == 1:\n",
    "    print(\"\\nStart DL training...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    history = model_py.fit(\n",
    "        xtrain, Y_train,\n",
    "        validation_data=(xval, Y_val),\n",
    "        #train_dataset, \n",
    "        #validation_data=val_dataset\n",
    "        batch_size=mini_batch_size,\n",
    "        epochs=max_epochs,\n",
    "        shuffle=True,  # Shuffle data at each epoch\n",
    "        callbacks=[lr_scheduler, tensorboard_callback],\n",
    "        validation_freq=validationFrequency,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Training completed in {elapsed_time / 60:.2f} minutes.\")\n",
    "\n",
    "    # Save the trained model\n",
    "    #The saved .keras file contains:\n",
    "    # - The model's configuration (architecture)\n",
    "    # - The model's weights\n",
    "    # - The model's optimizer's state (if any)\n",
    "    # model.save() is an alias for keras.saving.save_model()\n",
    "    model_py.save(saved_models + model_type + '.keras')  # The file needs to end with the .keras extension\n",
    "\n",
    "    print(f\"\\nModel saved in {saved_models}\")\n",
    "\n",
    "    #np.save(os.path.join(output_folder, 'history.npy'), history.history)\n",
    "    #np.save(os.path.join(output_folder, 'Y_predicted.npy'), Y_predicted)\n",
    "    #print(\"History and Y_predicted saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeaff216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start DL prediction...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step\n",
      "502\n",
      "571\n",
      "\n",
      "Rate_OPT: 1.7807620763778687\n",
      "Rate_DL: 1.0507421493530273\n",
      "Rate_DL_py: 1.6119709014892578\n"
     ]
    }
   ],
   "source": [
    "## DL Model Prediction\n",
    "# # SOSTITUIRE X_val CON X_test!!!\n",
    "Rate_DL_py = model_predict(xtest, model_py, YValidation_un2)\n",
    "\n",
    "# Scrittura in formato HDF5 (compatibile MATLAB v7.3)\n",
    "with h5py.File(filename_Rate_DL_py, 'w') as f:\n",
    "    f.create_dataset('Rate_DL_py', data=Rate_DL_py)\n",
    "\n",
    "filename_Rate_OPT = network_folder_in + 'Rate_OPT' + end_folder_Training_Size_dd + '.mat'\n",
    "\n",
    "with h5py.File(filename_Rate_OPT, 'r') as f:\n",
    "    Rate_OPT = f['Rate_OPT'][:][0][0]\n",
    "\n",
    "filename_Rate_DL = network_folder_in + 'Rate_DL' + end_folder_Training_Size_dd + '.mat'\n",
    "\n",
    "with h5py.File(filename_Rate_DL, 'r') as f:\n",
    "    Rate_DL = f['Rate_DL'][:][0][0]\n",
    "\n",
    "# Output finali\n",
    "print(f\"\\nRate_OPT: {Rate_OPT}\")\n",
    "print(f\"Rate_DL: {Rate_DL}\")\n",
    "if load_model_flag == 1:\n",
    "    print(f\"Rate_DL_py_load: {Rate_DL_py_load}\")\n",
    "print(f\"Rate_DL_py: {Rate_DL_py}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867da00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "deepmimo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
